{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGDzuoBxY6sm"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install pydot\n",
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_CDPw5bZLVd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmpylwr1ZM9d"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "model_checkpoint = 'bert-base-cased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2R16Wo4ZXOY"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "bert_model = TFBertModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cm(preds, true_label):\n",
        "  t = {0:'anger', 1:'fear', 2:'joy', 3:'love', 4:'neutral', 5:'sadness', 6:'surprise'}\n",
        "  cm = confusion_matrix(true_label, preds)\n",
        "  plt.figure(figsize=(10,7))\n",
        "  sns.heatmap(\n",
        "      cm, annot=True, xticklabels=t.values(),\n",
        "        yticklabels=t.values())\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"True\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ss7woXBho8Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmy0lEfSZZPg"
      },
      "outputs": [],
      "source": [
        "max_length = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Standardized BERT model (selected after tuning hidden dimension size) to try out with all different sizes"
      ],
      "metadata": {
        "id": "pytcrlPdlbnR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHHN4Cqld5m4"
      },
      "outputs": [],
      "source": [
        "def create_bert_multiclass_model(checkpoint = model_checkpoint,\n",
        "                                 num_classes = 7,\n",
        "                                 hidden_size = 201, \n",
        "                                 dropout=0.3,\n",
        "                                 learning_rate=0.00005):\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint)                                              \n",
        "    max_length = 200\n",
        "    bert_model.trainable = True\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}    \n",
        "    bert_out = bert_model(bert_inputs)\n",
        "    cls_token = bert_out[0][:, 0, :]\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden) \n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(hidden)\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
        "                                 metrics='accuracy') \n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S13cW__fezWv"
      },
      "outputs": [],
      "source": [
        "bert_model = create_bert_multiclass_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9knHspfOfceq"
      },
      "outputs": [],
      "source": [
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Phase 2: Test for optimal training data size for BERT"
      ],
      "metadata": {
        "id": "7PNxUnUGlMiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size = 17 k rows"
      ],
      "metadata": {
        "id": "kRgxoIsBlTdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh53XInIZdH1"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_17k_5k.csv')\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_17k_5k.csv\")\n",
        "test = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gQ0ab8WZy2G"
      },
      "outputs": [],
      "source": [
        "train_text = train['text'].tolist()\n",
        "val_text = val['text'].tolist()\n",
        "test_text = test['text'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8oWCftBc-Yj"
      },
      "outputs": [],
      "source": [
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XLLpcuTaD9x"
      },
      "outputs": [],
      "source": [
        "nptrain_labels = np.asarray(train['emotions'])\n",
        "npval_labels = np.asarray(val['emotions'])\n",
        "nptest_labels = np.asarray(test['emotions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rln8wzLxBpSY"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train['emotions']).codes\n",
        "npval_labels = pd.Categorical(val['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test['emotions']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m29Y0YWZfdEw"
      },
      "outputs": [],
      "source": [
        "bert_model_17 = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n"
      ],
      "metadata": {
        "id": "eH3Xp53a246G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ],
      "metadata": {
        "id": "cYnOndxrIb01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size = 34 k training rows"
      ],
      "metadata": {
        "id": "H_F6u91Ql-F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test with 34k\n",
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_34k_10k.csv')\n",
        "#lexicon = pd.read_csv(\"lexicon_220306.csv\")\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_34k_10k.csv\")\n",
        "test = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")"
      ],
      "metadata": {
        "id": "BxBQ5snEMcDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = train['text'].tolist()\n",
        "val_text = val['text'].tolist()\n",
        "test_text = test['text'].tolist()\n",
        "\n",
        "\n",
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "W1NDBUJtMo7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.Categorical(train['emotions']).codes\n",
        "npval_labels = pd.Categorical(val['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test['emotions']).codes"
      ],
      "metadata": {
        "id": "GJiSbYd_Mxm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_34k_history = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2)  "
      ],
      "metadata": {
        "id": "qbMMrNzHM8zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))"
      ],
      "metadata": {
        "id": "s1mtsnSENE3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size = 68 k training rows"
      ],
      "metadata": {
        "id": "trWzOCSlmEvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#try with 68 k\n",
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_68k_20k.csv')\n",
        "#lexicon = pd.read_csv(\"lexicon_220306.csv\")\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_68k_20k.csv\")\n",
        "test = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")"
      ],
      "metadata": {
        "id": "9Dlj_xGnzlWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = train['text'].tolist()\n",
        "val_text = val['text'].tolist()\n",
        "test_text = test['text'].tolist()\n",
        "\n",
        "\n",
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "Eo-hzFuRH4Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.Categorical(train['emotions']).codes\n",
        "npval_labels = pd.Categorical(val['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test['emotions']).codes"
      ],
      "metadata": {
        "id": "Hf5vbm0tH6kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_68 = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2)  "
      ],
      "metadata": {
        "id": "CR77Og8eH9r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))"
      ],
      "metadata": {
        "id": "RRriwjMMIB8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that using the 17k training size is enough for our purposes. We will use this for all other experiments."
      ],
      "metadata": {
        "id": "9juiIgghmVYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 2: Experimentation with Lexicon with BERT Model"
      ],
      "metadata": {
        "id": "dFKJInzymJr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filtering out training set to only include words in Lexicon."
      ],
      "metadata": {
        "id": "xVx2_9SRmf-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiWzEbChOqtr"
      },
      "outputs": [],
      "source": [
        "from nltk import WhitespaceTokenizer\n",
        "from nltk import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "w_tokenizer = WhitespaceTokenizer()\n",
        "def stringify(text):\n",
        "  finallist = ''\n",
        "  for i in range(len(text)):\n",
        "    if i + 1 == len(text):\n",
        "      finallist += text[i]\n",
        "    else:\n",
        "      finallist += text[i] + ' '\n",
        "  return finallist\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "def tokenizer(text):\n",
        "  return w_tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPwYAYvTkxYh"
      },
      "outputs": [],
      "source": [
        "#now doing a lexiconned version\n",
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_17k_5k.csv')\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_17k_5k.csv\")\n",
        "test = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")\n",
        "lexicon = pd.read_csv(\"lexicon_220306.csv\")\n",
        "lexicon_word_list  = lexicon['cleaned_stopwords'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnfvrXbbiMGW"
      },
      "outputs": [],
      "source": [
        "train['tokenized'] = train['text'].apply(tokenizer)\n",
        "train['lexiconned'] = train['tokenized'].apply(lambda x: [word for word in x if word in (lexicon_word_list)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJnSRvDYiNxi"
      },
      "outputs": [],
      "source": [
        "test['tokenized'] = test['text'].apply(tokenizer)\n",
        "test['lexiconned'] = test['tokenized'].apply(lambda x: [word for word in x if word in (lexicon_word_list)])\n",
        "val['tokenized'] = val['text'].apply(tokenizer)\n",
        "val['lexiconned'] = val['tokenized'].apply(lambda x: [word for word in x if word in (lexicon_word_list)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKUjR2sfiTEK"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTYkC6NeQXW1"
      },
      "outputs": [],
      "source": [
        "train['stringed'] = train['lexiconned'].apply(stringify)\n",
        "test['stringed']= test['lexiconned'].apply(stringify)\n",
        "val['stringed'] = val['lexiconned'].apply(stringify)\n",
        "train_x = train['stringed']\n",
        "train_y = train['emotions']\n",
        "val_x = val['stringed']\n",
        "val_y = val['emotions']\n",
        "test_x = test['stringed']\n",
        "test_y = test['emotions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9varV3AQ99I"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train_y).codes\n",
        "npval_labels = pd.Categorical(val_y).codes\n",
        "nptest_labels = pd.Categorical(test_y).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIDh0Zr3Pxmm"
      },
      "outputs": [],
      "source": [
        "train_text = train_x.tolist()\n",
        "val_text = val_x.tolist()\n",
        "test_text = test_x.tolist()\n",
        "max_length = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy4xkoOgQpDg"
      },
      "outputs": [],
      "source": [
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhOVUpWKQyEF"
      },
      "outputs": [],
      "source": [
        "bert_model_lexicon = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nUq0Y4cRGyd"
      },
      "outputs": [],
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter out training set to only include words NOT in Lexicon"
      ],
      "metadata": {
        "id": "8kgZV3elmnDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_17k_5k.csv')\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_17k_5k.csv\")\n",
        "test = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")\n",
        "lexicon = pd.read_csv(\"lexicon_220306.csv\")\n",
        "lexicon_word_list  = lexicon['cleaned_stopwords'].tolist()"
      ],
      "metadata": {
        "id": "pxAn6cn6HntH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK4XVsjrU0yi"
      },
      "outputs": [],
      "source": [
        "# now only words not in lexicon\n",
        "train['tokenized'] = train['text'].apply(tokenizer)\n",
        "train['lexiconned'] = train['tokenized'].apply(lambda x: [word for word in x if word not in (lexicon_word_list)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S_fBy35nQbg"
      },
      "outputs": [],
      "source": [
        "test['tokenized'] = test['text'].apply(tokenizer)\n",
        "test['lexiconned'] = test['tokenized'].apply(lambda x: [word for word in x if word not in (lexicon_word_list)])\n",
        "val['tokenized'] = val['text'].apply(tokenizer)\n",
        "val['lexiconned'] = val['tokenized'].apply(lambda x: [word for word in x if word not in (lexicon_word_list)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujwjoU5XnTos"
      },
      "outputs": [],
      "source": [
        "train['stringed'] = train['lexiconned'].apply(stringify)\n",
        "test['stringed']= test['lexiconned'].apply(stringify)\n",
        "val['stringed'] = val['lexiconned'].apply(stringify)\n",
        "train_x = train['stringed']\n",
        "train_y = train['emotions']\n",
        "val_x = val['stringed']\n",
        "val_y = val['emotions']\n",
        "test_x = test['stringed']\n",
        "test_y = test['emotions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KmmKxR0nXYL"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train_y).codes\n",
        "npval_labels = pd.Categorical(val_y).codes\n",
        "nptest_labels = pd.Categorical(test_y).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfhdbv5nZLW"
      },
      "outputs": [],
      "source": [
        "train_text = train_x.tolist()\n",
        "val_text = val_x.tolist()\n",
        "test_text = test_x.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrGRTpFMna6C"
      },
      "outputs": [],
      "source": [
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvRsKO6wnc6H"
      },
      "outputs": [],
      "source": [
        "bert_model_reverse_lexicon = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nbZe56nk4q"
      },
      "outputs": [],
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Phase 2: Test best BERT model on Kaggle data set"
      ],
      "metadata": {
        "id": "F0IprCRlmwKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-BTm54RnoUq"
      },
      "outputs": [],
      "source": [
        " #test with new data set with no lexicon\n",
        "train = pd.read_csv('train_data_imbalanced_stratified_no_dupe_17k_5k.csv')\n",
        "val = pd.read_csv(\"validation_data_imbalanced_stratified_no_dupe_17k_5k.csv\")\n",
        "test = pd.read_csv(\"kaggle_data_no_dupe.csv\")\n",
        "train_text = train['text'].tolist()\n",
        "val_text = val['text'].tolist()\n",
        "test_text = test['text'].tolist()\n",
        "\n",
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN6XnSklX_q9"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train['emotions']).codes\n",
        "npval_labels = pd.Categorical(val['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test['emotions']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY0DwF9sYBF8"
      },
      "outputs": [],
      "source": [
        "bert_model_kaggle = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTTTSRu_YCwU"
      },
      "outputs": [],
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test on balanced clean test set"
      ],
      "metadata": {
        "id": "qmVUkvxvxKrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test_data_balanced_no_dupe.csv\")\n",
        "test_text = test['text'].tolist()\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "nptest_labels = pd.Categorical(test['emotions']).codes"
      ],
      "metadata": {
        "id": "2XokthFcxN3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ],
      "metadata": {
        "id": "xnTv60C1xffC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 3: Create combined training set with CARER and Kaggle data, test on BERT architecture"
      ],
      "metadata": {
        "id": "y5CGlVX9m4Hi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvEHXaZmTVtw"
      },
      "outputs": [],
      "source": [
        "#try a new bert with mixed kaggle and clean set with squished labeles\n",
        "train_mix = pd.read_csv(\"train_20k_12k_mixed_strat_clean_kaggle_230404.csv\")\n",
        "val_mix = pd.read_csv(\"validation_20k_12k_mixed_strat_clean_kaggle_230404.csv\")\n",
        "test_mix = pd.read_csv(\"test_data_imbalanced_stratified_no_dupe.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dupes = pd.merge(train_mix, test_mix, on='text')\n",
        "to_drop = train_mix['text'].isin(dupes['text'])\n",
        "train_mix = train_mix[~to_drop]\n",
        "train_mix.shape"
      ],
      "metadata": {
        "id": "xLUfdSxQxCvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNM1_P6UTySV"
      },
      "outputs": [],
      "source": [
        "train_text = train_mix['text'].tolist()\n",
        "val_text = val_mix['text'].tolist()\n",
        "test_text = test_mix['text'].tolist()\n",
        "\n",
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKp5D4PET7Pc"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train_mix['emotions']).codes\n",
        "npval_labels = pd.Categorical(val_mix['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test_mix['emotions']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmmsVemPT9SW"
      },
      "outputs": [],
      "source": [
        "bert_model_mix = bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npval_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=2) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predictions = tf.argmax(predictions, axis=-1)\n",
        "print(classification_report(nptest_labels, predictions.numpy()))\n",
        "cm(nptest_labels, predictions.numpy())"
      ],
      "metadata": {
        "id": "qVuUxhkb2c3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's see if Bert can tell the different between the kaggle and clean data sources. (Needs tuning)"
      ],
      "metadata": {
        "id": "kt6ntv1LVpTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bert_model(max_sequence_length=200,\n",
        "                          hidden_size = 50, \n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005):\n",
        "    bert_model = TFBertModel.from_pretrained('bert-base-cased')\n",
        "    bert_model.trainable = True\n",
        "    input_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "    cls_token = bert_out[0][:, 0, :]\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden) \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification]) \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "                                 metrics='accuracy')\n",
        "    return classification_model"
      ],
      "metadata": {
        "id": "mZTwit1mVoqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8yc7ADWm_el"
      },
      "outputs": [],
      "source": [
        "train_mix = pd.read_csv(\"train_source_20k_12k_mixed_230404.csv\")\n",
        "test_mix = pd.read_csv(\"validation_source_20k_12k_mixed_230404.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mix.head()"
      ],
      "metadata": {
        "id": "MzzdPa58ahCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SA2DF8UnHNg"
      },
      "outputs": [],
      "source": [
        "train_text = train_mix['text'].tolist()\n",
        "#val_text = val['text'].tolist()\n",
        "test_text = test_mix['text'].tolist()\n",
        "\n",
        "train_encodings = bert_tokenizer(train_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "#valid_encodings = bert_tokenizer(val_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe1hJ5xMnI-X"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.Categorical(train_mix['source']).codes\n",
        "#npval_labels = pd.Categorical(val['emotions']).codes\n",
        "nptest_labels = pd.Categorical(test_mix['source']).codes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_encodings)"
      ],
      "metadata": {
        "id": "tLgCoXb0XoBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVZy5oAGrulH"
      },
      "outputs": [],
      "source": [
        "bert_model_source = create_bert_cls_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grWQmL7pnL6g"
      },
      "outputs": [],
      "source": [
        "bert_source_model_history = bert_model_source.fit(\n",
        "    [train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "    train_labels, validation_data=([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask], \n",
        "    nptest_labels), batch_size=8, epochs=2) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}